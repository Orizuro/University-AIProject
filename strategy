Store every (95 or x) number of population

Initial Population


Fitness Function
    Total of balls that hit the brick divided by the total of the baals that hit y = 0 ;

Mutation Algorithm

    1.Selection
        Depends of the fitness
    [fitness: 0-80]
        Tournament Selection{
            • São selecionados K indíviduos de uma população de forma aleatória
            • Os melhores x desses serão progenitores
            • Repete-se o processo para selecionar o novo progenitor.
            (Slide 9 do PPT aula 20)
        }
    [fitness: 80-100]
        Rank Selection{
            • Usado principlamente quando os valores de fitness dos indivíduos estão muito próximos (O que
            acontece tipicamente no fim da procura).
            • A roleta terá regiões iguais para cada indvíduo, o que significa que cada um pode ser selecionado com a mesma
            probabilidade.
            • Abordagem pouco elitista.
        }

    2.CrossOver
    K - Point Crossover (K= 3):


    3.Mutation
        Scramble Mutation
        10 to 20 % of the array in a random location


The Hidden Layers

So those few rules set the number of layers and size (neurons/layer) for both the input and output layers. That leaves the hidden layers.

How many hidden layers? Well, if your data is linearly separable (which you often know by the time you begin coding a NN), then you don't need any hidden layers at all. Of course, you don't need an NN to resolve your data either, but it will still do the job.

Beyond that, as you probably know, there's a mountain of commentary on the question of hidden layer configuration in NNs (see the insanely thorough and insightful NN FAQ for an excellent summary of that commentary). One issue within this subject on which there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very few. One hidden layer is sufficient for the large majority of problems.

So what about the size of the hidden layer(s)--how many neurons? There are some empirically derived rules of thumb; of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'. Jeff Heaton, the author of Introduction to Neural Networks in Java, offers a few more.

In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) the number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers.